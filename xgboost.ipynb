{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30daeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anair26\\AppData\\Local\\Temp\\ipykernel_7140\\4092739941.py:17: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  prices = yf.download(selected_etfs, start = '2010-01-01', end = '2019-12-31')['Close']\n",
      "[*********************100%***********************]  7 of 7 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1064 entries, 2015-10-08 to 2019-12-30\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ARKK    1064 non-null   float64\n",
      " 1   ARKW    1064 non-null   float64\n",
      " 2   SHY     1064 non-null   float64\n",
      " 3   VGSH    1064 non-null   float64\n",
      " 4   VPU     1064 non-null   float64\n",
      " 5   XLRE    1064 non-null   float64\n",
      " 6   XLU     1064 non-null   float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 66.5 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "selected_etfs = ['XLU','XLRE','VPU','SHY','VGSH','ARKK','ARKW']\n",
    "\n",
    "prices = yf.download(selected_etfs, start = '2015-01-01', end = '2019-12-31')['Close']\n",
    "\n",
    "pair1 = ['XLU','XLRE']\n",
    "pair2 = ['VPU','XLRE']\n",
    "pair3 = ['SHY','VGSH']\n",
    "pair4 = ['ARKK','ARKW']\n",
    "\n",
    "prices = prices.dropna()\n",
    "\n",
    "prices.info()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de00cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(z, upper=1.0, lower=-1.0):\n",
    "    return np.where(z > upper, -1,    # SHORT spread\n",
    "           np.where(z < lower, 1,     # LONG spread\n",
    "           0))                        # No trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada13a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Date, pair, z_score, mean, std, spread, label]\n",
      "Index: []\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   Date     0 non-null      datetime64[ns]\n",
      " 1   pair     0 non-null      object        \n",
      " 2   z_score  0 non-null      float64       \n",
      " 3   mean     0 non-null      float64       \n",
      " 4   std      0 non-null      float64       \n",
      " 5   spread   0 non-null      float64       \n",
      " 6   label    0 non-null      int64         \n",
      "dtypes: datetime64[ns](1), float64(4), int64(1), object(1)\n",
      "memory usage: 132.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_pairs = [pair1,pair2,pair3,pair4]\n",
    "\n",
    "# store the spread series for every pair\n",
    "spreads = {}\n",
    "\n",
    "for pair in all_pairs:\n",
    "\n",
    "    a = prices[pair[0]]\n",
    "    b = prices[pair[1]]\n",
    "\n",
    "    X = sm.add_constant(b)\n",
    "    model = sm.OLS(a, X).fit()\n",
    "    hedge_ratio = model.params[pair[1]]\n",
    "                               \n",
    "    spread = a - hedge_ratio * b\n",
    "\n",
    "    # Normalize the spread\n",
    "\n",
    "    lookback = 30\n",
    "\n",
    "    spread_mean = spread.rolling(window=lookback).mean()\n",
    "    spread_std = spread.rolling(window=lookback).std()\n",
    "\n",
    "    spreads['rolling_std'] = spread_std\n",
    "    spreads['rolling_mean'] = spread_mean\n",
    "\n",
    "    z_spread = (spread - spread_mean) / spread_std\n",
    "\n",
    "    spreads[f\"{pair[0]}-{pair[1]}\"] = z_spread\n",
    "\n",
    "\n",
    "    pair_name = f\"{pair[0]}-{pair[1]}\"\n",
    "    spreads[f\"{pair_name}_spread\"] = spread\n",
    "    spreads[f\"{pair_name}_z\"] = z_spread\n",
    "    spreads[f\"{pair_name}_mean\"] = spread_mean\n",
    "    spreads[f\"{pair_name}_std\"] = spread_std\n",
    "\n",
    "\n",
    "# Step 1: Create spread_df like before\n",
    "spread_df = pd.DataFrame(spreads)  # columns are pairs, index is datetime\n",
    "\n",
    "spread_df.dropna(inplace=True)\n",
    "spread_df.reset_index(inplace=True)\n",
    "\n",
    "melted_z = spread_df.melt(id_vars='Date', value_name='z_score', var_name='pair')\n",
    "melted_z = melted_z[melted_z['pair'].str.endswith('_z')].copy()\n",
    "melted_z['pair'] = melted_z['pair'].str.replace('_z', '', regex=False)\n",
    "\n",
    "melted_mean = spread_df.melt(id_vars='Date', value_name='mean', var_name='pair')\n",
    "melted_mean = melted_mean[melted_mean['pair'].str.endswith('_mean')].copy()\n",
    "melted_mean['pair'] = melted_mean['pair'].str.replace('_mean', '', regex=False)\n",
    "\n",
    "melted_std = spread_df.melt(id_vars='Date', value_name='std', var_name='pair')\n",
    "melted_std = melted_std[melted_std['pair'].str.endswith('_std')].copy()\n",
    "melted_std['pair'] = melted_mean['pair'].str.replace('_std', '', regex=False)\n",
    "\n",
    "melted_spread = spread_df.melt(id_vars='Date', value_name='spread', var_name='pair')\n",
    "melted_spread = melted_spread[melted_spread['pair'].str.endswith('_spread')].copy()\n",
    "melted_spread['pair'] = melted_spread['pair'].str.replace('_spread', '', regex=False)\n",
    "\n",
    "spread_long = melted_z.merge(melted_mean, on=['Date', 'pair'])\n",
    "spread_long = spread_long.merge(melted_std, on=['Date', 'pair'])\n",
    "spread_long = spread_long.merge(melted_spread, on=['Date', 'pair'])\n",
    "\n",
    "# Now each row is a single (pair, date, z_score)\n",
    "\n",
    "spread_long['label'] = generate_labels(spread_long['z_score'])\n",
    "\n",
    "print(spread_long.head())\n",
    "\n",
    "spread_long.info()\n",
    "spread_long['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a11728",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spread_long' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# create features\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m spread_long[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_score_lag1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mspread_long\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m spread_long[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspread_lag1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m spread_long\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspread\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m spread_long[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_score_lag2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m spread_long\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spread_long' is not defined"
     ]
    }
   ],
   "source": [
    "# create features\n",
    "\n",
    "\n",
    "spread_long['z_score_lag1'] = spread_long.groupby('pair')['z_score'].shift(1)\n",
    "spread_long['spread_lag1'] = spread_long.groupby('pair')['spread'].shift(1)\n",
    "spread_long['z_score_lag2'] = spread_long.groupby('pair')['z_score'].shift(2)\n",
    "spread_long['spread_lag2'] = spread_long.groupby('pair')['spread'].shift(2)\n",
    "spread_long['z_score_lag3'] = spread_long.groupby('pair')['z_score'].shift(3)\n",
    "spread_long['spread_lag3'] = spread_long.groupby('pair')['spread'].shift(3)\n",
    "spread_long['z_score_lag4'] = spread_long.groupby('pair')['z_score'].shift(4)\n",
    "spread_long['spread_lag4'] = spread_long.groupby('pair')['spread'].shift(4)\n",
    "\n",
    "spread_long['rolling_std'] = spread_long.groupby('pair')['std'].rolling(lookback).std().reset_index(level=0, drop=True)\n",
    "spread_long['rolling_mean'] = spread_long.groupby('pair')['mean'].rolling(lookback).mean().reset_index(level=0, drop=True)\n",
    "spread_long['volatility'] = spread_long.groupby('pair')['z_score'].rolling(lookback).std().reset_index(level=0, drop=True)\n",
    "\n",
    "\n",
    "# Drop NaNs\n",
    "spread_long.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6ebd4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   Date          0 non-null      datetime64[ns]\n",
      " 1   pair          0 non-null      object        \n",
      " 2   z_score       0 non-null      float64       \n",
      " 3   mean          0 non-null      float64       \n",
      " 4   std           0 non-null      float64       \n",
      " 5   spread        0 non-null      float64       \n",
      " 6   label         0 non-null      int64         \n",
      " 7   z_score_lag1  0 non-null      float64       \n",
      " 8   spread_lag1   0 non-null      float64       \n",
      " 9   rolling_std   0 non-null      float64       \n",
      " 10  rolling_mean  0 non-null      float64       \n",
      " 11  volatility    0 non-null      float64       \n",
      " 12  pair_encoded  0 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(10), int64(1), object(1)\n",
      "memory usage: 132.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "spread_long['pair_encoded'] = le.fit_transform(spread_long['pair'])\n",
    "\n",
    "spread_long.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a92fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pair_encoded','z_score_lag1','rolling_std','rolling_mean','spread','spread_lag1','z_score','volatility']\n",
    "target = 'label'\n",
    "\n",
    "X = spread_long[features]\n",
    "y = spread_long[target]\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False  # shuffle=False preserves time order\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',  # For multi-class (long, short, neutral)\n",
    "    num_class=3,\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=100,        # Number of boosting rounds (trees)\n",
    "    learning_rate=0.1,       # Step size shrinkage (how much model learns each round)\n",
    "    max_depth=3,             # Max depth of a tree (controls complexity)\n",
    "    subsample=0.8,           # Row subsampling for regularization\n",
    "    colsample_bytree=0.8,    # Feature subsampling\n",
    "    objective='multi:softmax', # Use 'multi:softmax' for multiclass (e.g., long/short/neutral)\n",
    "    num_class=3,             # Number of classes in your labels\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4582efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBClassifier(objective='multi:softmax', num_class=3),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
